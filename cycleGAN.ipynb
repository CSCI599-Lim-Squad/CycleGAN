{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scipy.io\n",
    "import scipy.misc\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv2d(input, kernel_size, stride, num_filter, name = 'conv2d'):\n",
    "    with tf.variable_scope(name):\n",
    "        stride_shape = [1, stride, stride, 1]\n",
    "        filter_shape = [kernel_size, kernel_size, input.get_shape()[3], num_filter]\n",
    "\n",
    "        W = tf.get_variable('w', filter_shape, tf.float32, tf.random_normal_initializer(0.0, 0.02))\n",
    "        b = tf.get_variable('b', [1, 1, 1, num_filter], initializer = tf.constant_initializer(0.0))\n",
    "        return tf.nn.conv2d(input, W, stride_shape, padding = 'SAME') + b\n",
    "\n",
    "def conv2d_transpose(input, kernel_size, stride, num_filter, name = 'conv2d_transpose'):\n",
    "    with tf.variable_scope(name):\n",
    "        stride_shape = [1, stride, stride, 1]\n",
    "        filter_shape = [kernel_size, kernel_size, num_filter, input.get_shape()[3]]\n",
    "        output_shape = tf.stack([tf.shape(input)[0], tf.shape(input)[1] * 2, tf.shape(input)[2] * 2, num_filter])\n",
    "\n",
    "        W = tf.get_variable('w', filter_shape, tf.float32, tf.random_normal_initializer(0.0, 0.02))\n",
    "        b = tf.get_variable('b', [1, 1, 1, num_filter], initializer = tf.constant_initializer(0.0))\n",
    "        return tf.nn.conv2d_transpose(input, W, output_shape, stride_shape, padding = 'SAME') + b\n",
    "\n",
    "def fc(input, num_output, name = 'fc'):\n",
    "    with tf.variable_scope(name):\n",
    "        num_input = input.get_shape()[1]\n",
    "        W = tf.get_variable('w', [num_input, num_output], tf.float32, tf.random_normal_initializer(0.0, 0.02))\n",
    "        b = tf.get_variable('b', [num_output], initializer = tf.constant_initializer(0.0))\n",
    "        return tf.matmul(input, W) + b\n",
    "\n",
    "def batch_norm(input, is_training):\n",
    "    out = tf.contrib.layers.batch_norm(input, decay = 0.99, center = True, scale = True,\n",
    "                                       is_training = is_training, updates_collections = None)\n",
    "    return out\n",
    "\n",
    "def leaky_relu(input, alpha = 0.2):\n",
    "    return tf.maximum(alpha * input, input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class cycleGAN(object):\n",
    "    def __init__(self):\n",
    "        #initiater\n",
    "        self.num_epoch = 10\n",
    "        self.batch_size = 32\n",
    "        self.log_step = 1\n",
    "        self.visualize_step = 200\n",
    "        self.code_size = 64\n",
    "        self.learning_rate = 1e-4\n",
    "        \n",
    "        self.dis_name_1 = 'dis1'\n",
    "        self.dis_name_2 = 'dis2'\n",
    "        self.gen_name_1_to_2 = 'gen_1_to_2'\n",
    "        self.gen_name_2_to_1 = 'gen_2_to_1'\n",
    "\n",
    "        self.reuse = {\n",
    "            self.dis_name_1: False,\n",
    "            self.dis_name_2: False,\n",
    "            self.gen_name_1_to_2: False,\n",
    "            self.gen_name_2_to_1: False\n",
    "        }\n",
    "        \n",
    "        self.batch_size = 20\n",
    "        \n",
    "        self.lamda = 0.5\n",
    "\n",
    "        self.input1 = tf.placeholder(tf.float32, [None, 32, 32, 3])\n",
    "        self.input2 = tf.placeholder(tf.float32, [None, 32, 32, 3])\n",
    "\n",
    "        self.real_label = tf.placeholder(tf.float32, [None, 1])\n",
    "        self.fake_label = tf.placeholder(tf.float32, [None, 1])\n",
    "\n",
    "        self.is_train = tf.placeholder(tf.bool)\n",
    "\n",
    "        self._init_ops()\n",
    "        \n",
    "    def _discriminator(self, input, scopeName):\n",
    "        #initiate discriminator for a certain scope\n",
    "        with tf.variable_scope(scopeName, reuse = self.reuse[scopeName]):\n",
    "            self.reuse[scopeName] = True\n",
    "            dis_conv1 = conv2d(input, 4, 2, 32, 'conv1')\n",
    "            dis_lrelu1 = leaky_relu(dis_conv1)\n",
    "            dis_conv2 = conv2d(dis_lrelu1, 4, 2, 64, 'conv2')\n",
    "            dis_batchnorm2 = batch_norm(dis_conv2, self.is_train)\n",
    "            dis_lrelu2 = leaky_relu(dis_batchnorm2)\n",
    "            dis_conv3 = conv2d(dis_lrelu2, 4, 2, 128, 'conv3')\n",
    "            dis_batchnorm3 = batch_norm(dis_conv3, self.is_train)\n",
    "            dis_lrelu3 = leaky_relu(dis_batchnorm3)\n",
    "            dis_reshape3 = tf.reshape(dis_lrelu3, [-1, 4 * 4 * 128])\n",
    "            dis_fc4 = fc(dis_reshape3, 1, 'fc4')\n",
    "            return dis_fc4\n",
    "\n",
    "    \n",
    "    def _generator(self, input, scopeName):\n",
    "        #initiate generator for a certain scope\n",
    "        with tf.variable_scope(scopeName, reuse = self.reuse[scopeName]):\n",
    "            self.reuse[scopeName] = True\n",
    "            gen_conv2 = conv2d_transpose(input, 4, 2, 64, 'conv2')\n",
    "            gen_batchnorm2 = batch_norm(gen_conv2, self.is_train)\n",
    "            gen_lrelu2 = leaky_relu(gen_batchnorm2)\n",
    "            gen_conv3 = conv2d_transpose(gen_lrelu2, 4, 2, 32, 'conv3')\n",
    "            gen_batchnorm3 = batch_norm(gen_conv3, self.is_train)\n",
    "            gen_lrelu3 = leaky_relu(gen_batchnorm3)\n",
    "            gen_conv4 = conv2d_transpose(gen_lrelu3, 4, 2, 3, 'conv4')\n",
    "            gen_sigmoid4 = tf.sigmoid(gen_conv4)\n",
    "            return gen_sigmoid4\n",
    "    \n",
    "    def _adviserial_loss(self, logits, labels):\n",
    "        #binary L2 loss\n",
    "        return tf.square(logits - labels)\n",
    "        \n",
    "    def _cycle_loss(self, logits, labels):\n",
    "        #L1 loss\n",
    "        return tf.losses.absolute_difference(logits, labels)\n",
    "    \n",
    "    def _init_ops(self):\n",
    "        #operations\n",
    "        self.real_1_dis = self._discriminator(self.input1, self.dis_name_1)\n",
    "        self.real_2_dis = self._discriminator(self.input2, self.dis_name_2)\n",
    "        \n",
    "        self.generated_1 = self._generator(self.input2, self.gen_name_2_to_1)\n",
    "        self.generated_2 = self._generator(self.input1, self.gen_name_1_to_2)\n",
    "        \n",
    "        self.cycle_fake_1 = self._generator(self.generated_2, self.gen_name_2_to_1)\n",
    "        self.cycle_fake_2 = self._generator(self.generated_1, self.gen_name_1_to_2)\n",
    "        \n",
    "        self.fake_1_dis = self._discriminator(self.generated_1, self.dis_name_1)\n",
    "        self.fake_2_dis = self._discriminator(self.generated_2, self.dis_name_2)\n",
    "        \n",
    "        #variable scope\n",
    "        self.gen_1_to_2 = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,self.gen_name_1_to_2)\n",
    "        self.gen_2_to_1 = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,self.gen_name_2_to_1)\n",
    "        self.gen_scope = self.gen_1_to_2 + self.gen_2_to_1\n",
    "        \n",
    "        self.dis1 = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,self.dis_name_1)\n",
    "        self.dis2 = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,self.dis_name_2)\n",
    "        self.dis_scope = self.dis1 + self.dis2\n",
    "        \n",
    "        #loss functions\n",
    "        gan_loss_1 = self._adviserial_loss(self.real_1_dis, self.real_label) + self._adviserial_loss(self.fake_1_dis, self.fake_label)\n",
    "        \n",
    "        gan_loss_2 = self._adviserial_loss(self.real_2_dis, self.real_label) + self._adviserial_loss(self.fake_2_dis, self.fake_label)\n",
    "            \n",
    "        cycle_loss = self._cycle_loss(self.cycle_fake_1, self.input1) + self._cycle_loss(self.cycle_fake_2, self.input2)\n",
    "        \n",
    "        self.gen_loss = gan_loss_1 + gan_loss_2 + self.lamda*cycle_loss\n",
    "        \n",
    "        self.dis_loss = self._adviserial_loss(self.real_1_dis, self.real_label) + self._adviserial_loss(self.real_2_dis, self.real_label) \n",
    "        \n",
    "        #optimizers and training step\n",
    "        dis_optimizer = tf.train.RMSPropOptimizer(self.learning_rate)\n",
    "        self.dis_train_op = dis_optimizer.minimize(self.dis_loss, var_list = self.dis_scope)\n",
    "        \n",
    "        gen_optimizer = tf.train.RMSPropOptimizer(self.learning_rate)\n",
    "        self.gen_train_op = gen_optimizer.minimize(self.gen_loss, var_list = self.gen_scope)\n",
    "        \n",
    "        \n",
    "    def train(self, sess):\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        num_train = 10000\n",
    "        step = 0\n",
    "        \n",
    "        # smooth the loss curve so that it does not fluctuate too much\n",
    "        smooth_factor = 0.95\n",
    "        plot_dis_s = 0\n",
    "        plot_gen_s = 0\n",
    "        plot_ws = 0\n",
    "        \n",
    "        dis_losses = []\n",
    "        gen_losses = []\n",
    "        \n",
    "        print('start training')\n",
    "        for epoch in range(self.num_epoch):\n",
    "            for i in range(num_train // self.batch_size):\n",
    "                \n",
    "                step += 1\n",
    "\n",
    "                input1 = np.random.standard_normal([self.batch_size, 32,32,3])\n",
    "                input2 = np.random.exponential(1.0, [self.batch_size, 32,32,3])\n",
    "                \n",
    "                zeros = np.zeros([self.batch_size, 1])\n",
    "                ones = np.ones([self.batch_size, 1])\n",
    "        \n",
    "                ################################################################################\n",
    "                # Prob 2-1: complete the feed dictionary                                       #\n",
    "                ################################################################################\n",
    "                \n",
    "                dis_feed_dict = {self.input1: input1,\n",
    "                                 self.input2: input2,\n",
    "                                 self.real_label: ones,\n",
    "                                 self.fake_label: zeros,\n",
    "                                 self.is_train: True}\n",
    "        \n",
    "                ################################################################################\n",
    "                #                               END OF YOUR CODE                               #\n",
    "                ################################################################################\n",
    "\n",
    "                _, dis_loss = sess.run([self.dis_train_op, self.dis_loss], feed_dict = dis_feed_dict)\n",
    "                \n",
    "                input1 = np.random.standard_normal([self.batch_size, 32,32,3])\n",
    "                input2 = np.random.exponential(1.0, [self.batch_size, 32,32,3])\n",
    "        \n",
    "                ################################################################################\n",
    "                # Prob 2-1: complete the feed dictionary                                       #\n",
    "                ################################################################################\n",
    "                \n",
    "                gen_feed_dict = {self.input1: input1,\n",
    "                                 self.input2: input2,\n",
    "                                 self.real_label: ones,\n",
    "                                 self.fake_label: zeros,\n",
    "                                 self.is_train: True}\n",
    "        \n",
    "                ################################################################################\n",
    "                #                               END OF YOUR CODE                               #\n",
    "                ################################################################################\n",
    "\n",
    "                _, gen_loss = sess.run([self.gen_train_op, self.gen_loss], feed_dict = gen_feed_dict)\n",
    "\n",
    "                plot_dis_s = plot_dis_s * smooth_factor + dis_loss * (1 - smooth_factor)\n",
    "                plot_gen_s = plot_gen_s * smooth_factor + gen_loss * (1 - smooth_factor)\n",
    "                plot_ws = plot_ws * smooth_factor + (1 - smooth_factor)\n",
    "                dis_losses.append(plot_dis_s / plot_ws)\n",
    "                gen_losses.append(plot_gen_s / plot_ws)\n",
    "\n",
    "                if step % self.log_step == 0:\n",
    "                    print('Iteration {0}: dis loss = {1:.4f}, gen loss = {2:.4f}'.format(step, dis_loss, gen_loss))\n",
    "\n",
    "            plt.plot(dis_losses)\n",
    "            plt.title('discriminator loss')\n",
    "            plt.xlabel('iterations')\n",
    "            plt.ylabel('loss')\n",
    "            plt.show()\n",
    "\n",
    "            plt.plot(gen_losses)\n",
    "            plt.title('generator loss')\n",
    "            plt.xlabel('iterations')\n",
    "            plt.ylabel('loss')\n",
    "            plt.show()\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start training\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    with tf.device('/cpu:0'):\n",
    "        cycle_gan = cycleGAN()\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        cycle_gan.train(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
